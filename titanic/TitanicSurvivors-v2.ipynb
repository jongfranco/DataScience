{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster\n",
    "\n",
    "\n",
    "## Competition Description\n",
    "\n",
    "[Kaggle Titanic Competition](https://www.kaggle.com/c/titanic)\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this challenge I was asked to complete the analysis of what sorts of people were likely to survive. In particular, I applied the tools of machine learning to predict which passengers survived the tragedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with Decision Trees\n",
    "\n",
    "A decision tree automates the data slicing process for you and outputs a classification model or classifier.\n",
    "\n",
    "Conceptually, the decision tree algorithm starts with all the data at the root node and scans all the variables for the best one to split on. Once a variable is chosen, you do the split and go down one level (or one node) and repeat. The final nodes at the bottom of the decision tree are known as terminal nodes, and the majority vote of the observations in that node determine how to predict for new observations that end up in that terminal node.\n",
    "\n",
    "\n",
    "## Cleaning and Formatting Data\n",
    "Before we can begin constructing trees we need to get our hands dirty and clean the data so that we can use all the features available. In the first iteration, we saw that the Age variable had some missing value. Missingness is a whole subject with and in itself, but we will use a simple imputation technique where we substitute each missing value with the median of the all present values.\n",
    "\n",
    "Another problem is that the Sex and Embarked variables are categorical but in a non-numeric format. Thus, we will need to assign each class a unique integer so that Python can handle the information. Embarked also has some missing values which we should impute with the most common class of embarkation, which is \"S\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId    False\n",
      "Survived       False\n",
      "Pclass         False\n",
      "Name           False\n",
      "Sex            False\n",
      "Age            False\n",
      "SibSp          False\n",
      "Parch          False\n",
      "Ticket         False\n",
      "Fare           False\n",
      "Cabin           True\n",
      "Embarked       False\n",
      "dtype: bool\n",
      "PassengerId    False\n",
      "Pclass         False\n",
      "Name           False\n",
      "Sex            False\n",
      "Age            False\n",
      "SibSp          False\n",
      "Parch          False\n",
      "Ticket         False\n",
      "Fare           False\n",
      "Cabin           True\n",
      "Embarked       False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Import the Numpy library\n",
    "import numpy as np\n",
    "\n",
    "# Import 'tree' from scikit-learn library\n",
    "from sklearn  import tree\n",
    "\n",
    "\n",
    "# Load the train and test datasets to create two DataFrames\n",
    "train_data = \"train.csv\"\n",
    "train = pd.read_csv(train_data)\n",
    "\n",
    "test_data = \"test.csv\"\n",
    "test = pd.read_csv(test_data)\n",
    "\n",
    "# Convert the male and female groups to integer form\n",
    "train[\"Sex\"].loc[train[\"Sex\"] == \"male\"] = 0\n",
    "train[\"Sex\"].loc[train[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "test[\"Sex\"].loc[test[\"Sex\"] == \"male\"] = 0\n",
    "test[\"Sex\"].loc[test[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "# Impute the Embarked variable\n",
    "train[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n",
    "train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())\n",
    "\n",
    "#test[\"Embarked\"] = test[\"Embarked\"].fillna(\"S\")\n",
    "test[\"Age\"] = train[\"Age\"].fillna(test[\"Age\"].median())\n",
    "test[\"Fare\"] = train[\"Fare\"].fillna(test[\"Fare\"].median())\n",
    "\n",
    "# Convert the Embarked classes to integer form\n",
    "train[\"Embarked\"].loc[train[\"Embarked\"] == \"S\"] = 0\n",
    "train[\"Embarked\"].loc[train[\"Embarked\"] == \"C\"] = 1\n",
    "train[\"Embarked\"].loc[train[\"Embarked\"] == \"Q\"] = 2\n",
    "\n",
    "#test[\"Embarked\"].loc[test[\"Embarked\"] == \"S\"] = 0\n",
    "#test[\"Embarked\"].loc[test[\"Embarked\"] == \"C\"] = 1\n",
    "#test[\"Embarked\"].loc[test[\"Embarked\"] == \"Q\"] = 2\n",
    "\n",
    "print(train.isnull().any())\n",
    "print(test.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating A Decision Tree\n",
    "\n",
    "We will use the scikit-learn and numpy libraries to build a decision tree. scikit-learn can be used to create tree objects from the DecisionTreeClassifier class. The methods that we will use take numpy arrays as inputs and therefore we will need to create those from the DataFrame that we already have. We will need the following to build a decision tree\n",
    "\n",
    "target: A one-dimensional numpy array containing the target/response from the train data. (Survival in our case)\n",
    "features: A multidimensional numpy array containing the features/predictors from the train data. (ex. Sex, Age)\n",
    "\n",
    "One way to quickly see the result of our decision tree is to see the importance of the features that are included. This is done by requesting the .feature_importances_ attribute of the tree object. Another quick metric is the mean accuracy that we can compute using the .score() function with features_one and target as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12885059  0.31274009  0.24953791  0.30887141]\n",
      "0.977553310887\n"
     ]
    }
   ],
   "source": [
    "# Print the train data to see the available features\n",
    "# print(train)\n",
    "\n",
    "# Create the target and features numpy arrays: target, features_one\n",
    "target = train[\"Survived\"].values\n",
    "features_one = train[[\"Pclass\", \"Sex\", \"Age\", \"Fare\"]].values\n",
    "\n",
    "# Fit your first decision tree: my_tree_one\n",
    "my_tree_one = tree.DecisionTreeClassifier()\n",
    "my_tree_one = my_tree_one.fit(features_one, target)\n",
    "\n",
    "# Look at the importance and score of the included features\n",
    "print(my_tree_one.feature_importances_)\n",
    "print(my_tree_one.score(features_one, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "To send a submission to Kaggle we need to predict the survival rates for the observations in the test set. In the last exercise we created simple predictions based on a single subset. Luckily, with our decision tree, we can make use of some simple functions to \"generate\" our answer without having to manually perform subsetting.\n",
    "\n",
    "First, we make use of the .predict() method. We provide it the model (my_tree_one), the values of features from the dataset for which predictions need to be made (test). To extract the features we will need to create a numpy array in the same way as we did when training the model. However, we need to take care of a small but important problem first. There is a missing value in the Fare feature that needs to be imputed.\n",
    "\n",
    "Next, we need to make sure our output is in line with the submission requirements of Kaggle: a csv file with exactly 418 entries and two columns: PassengerId and Survived. Then use the code provided to make a new data frame using DataFrame(), and create a csv file using to_csv() method from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 22.0 7.25]\n",
      " [3 1 38.0 71.2833]\n",
      " [2 0 26.0 7.925]\n",
      " ..., \n",
      " [3 0 28.0 8.05]\n",
      " [3 0 34.0 32.5]\n",
      " [3 0 18.0 13.0]]\n",
      "[0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1\n",
      " 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0\n",
      " 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 1 1 0\n",
      " 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1\n",
      " 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0\n",
      " 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0\n",
      " 1 1 0 0 1 0 0 1 0 0 0]\n",
      "      Survived\n",
      "892          0\n",
      "893          1\n",
      "894          0\n",
      "895          1\n",
      "896          1\n",
      "897          0\n",
      "898          0\n",
      "899          1\n",
      "900          1\n",
      "901          0\n",
      "902          1\n",
      "903          0\n",
      "904          1\n",
      "905          0\n",
      "906          1\n",
      "907          1\n",
      "908          1\n",
      "909          0\n",
      "910          0\n",
      "911          1\n",
      "912          0\n",
      "913          0\n",
      "914          1\n",
      "915          1\n",
      "916          1\n",
      "917          0\n",
      "918          1\n",
      "919          0\n",
      "920          0\n",
      "921          0\n",
      "...        ...\n",
      "1280         0\n",
      "1281         0\n",
      "1282         1\n",
      "1283         1\n",
      "1284         0\n",
      "1285         0\n",
      "1286         0\n",
      "1287         1\n",
      "1288         1\n",
      "1289         0\n",
      "1290         0\n",
      "1291         0\n",
      "1292         0\n",
      "1293         0\n",
      "1294         1\n",
      "1295         0\n",
      "1296         0\n",
      "1297         0\n",
      "1298         0\n",
      "1299         1\n",
      "1300         1\n",
      "1301         0\n",
      "1302         0\n",
      "1303         1\n",
      "1304         0\n",
      "1305         0\n",
      "1306         1\n",
      "1307         0\n",
      "1308         0\n",
      "1309         0\n",
      "\n",
      "[418 rows x 1 columns]\n",
      "(418, 1)\n"
     ]
    }
   ],
   "source": [
    "# Impute the missing value with the median\n",
    "#test.loc[test.Fare[152]] = 0#test.Fare.median()\n",
    "\n",
    "# Extract the features from the test set: Pclass, Sex, Age, and Fare.\n",
    "test_features = test[[\"Pclass\", \"Sex\", \"Age\", \"Fare\"]].values\n",
    "\n",
    "print test_features\n",
    "\n",
    "# Make your prediction using the test set and print them.\n",
    "try:\n",
    " my_prediction = my_tree_one.predict(test_features)\n",
    "except ValueError, e:\n",
    " print e\n",
    "\n",
    "print(my_prediction)\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions\n",
    "PassengerId = np.array(test[\"PassengerId\"]).astype(int)\n",
    "my_solution = pd.DataFrame(my_prediction, PassengerId, columns = [\"Survived\"])\n",
    "print(my_solution)\n",
    "\n",
    "# Check that your data frame has 418 entries\n",
    "print(my_solution.shape)\n",
    "\n",
    "# Write your solution to a csv file with the name my_solution.csv\n",
    "my_solution.to_csv(\"my_solution_one.csv\", index_label = [\"PassengerId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
